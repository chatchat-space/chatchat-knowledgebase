# 有限马尔可夫过程

MDPs是决策序列的一种经典形式化模型，其中的行动不仅会影响当前的即时奖励，也会通过未来的奖励值来影响后续的情况和状态，即决策与环境的互动。

## 3.1 the agent-environment interface

agent和environment的界限：**不能被agent任意改变的事务，均认为其属于environment**如果agent能够改变环境的reward，那么agent将倾向于学习reward的表示而不是如何通过行动来获得reward。

## 3.2 goals and rewards

agent的目标是通过environment传入的reward信号来量化，即最大化奖励值的累积求和。

## return

定义$G_t$表示时间点t之后的期望返回值，一种简单的期望返回值为对未来的奖励值序列进行求和：$G_t=R_{t+1}+R_{t+1}+...+R_T$.

### episodic tasks

episodes:agent与环境交互过程的任意重复性交互过程。terminal state:每个子片段的一个特殊状态，其后续时间点被重置进行新片段。定义$\mathcal{S}$为非终止态的集合，$S^{+}$为全体集合。

### continuing tasks

对于连续式任务，$T=\infty$,从而$G_t$趋于无穷，导致agent无法根据返回值来进行比较学习，此时需要加入衰减系数$\gamma$,即

$$
G_t=\sum_{k=0}^{\infty}{\gamma^kR_{t+k+1}}
$$

## 3.4 unified notation for episodic and continuing tasks

对于片段式任务降入一种特殊的吸收态，其特点是状态的转移和交互都是在自身进行，且奖励值为0，则片段式和连续任务可统一为：

$$
G_t=\sum_{k=t+1}^{T}{\gamma^{k-t-1}R_{t+k+1}}
$$
,其中$\gamma=1,T=\infty$不会同时存在；

## 3.5 the markov property

$$
p(s',r|s,a)=P(s',r|S_0,A_0,R_1,...,S_t=s,A_t=a)
$$

当前状态包含了所有历史状态的信息。
其中，$\sum_{s'\in S}{\sum_{r\in R}{p(s',r|s,a)}}=1$。

## 2.6 MDP

如果一个强化学习任务满足马尔可夫性质，我们称之为马尔可夫决策过程（MDP），如果**状态空间和行动空间都是有限的**，我们称之为有限马尔可夫过程（finite MDP).
根据$p(s',r|s,a)=P\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\}$我们可以得到：

* 状态转移概率

$$
p(s'|s,a)=P(S_t=s'|S_{t-1}=s,A_{t-1}=a)=\sum_{r\in R}{p(s',r|s,a)}
$$

* 给定状态、行动的期望值

$$
r(s,a)=E[R_t|S_{t-1}=s,A_{t-1}=a]=\sum_{r\in R}{r\sum_{s'\in S}{p(s',r|s,a)}}
$$

* 给定状态、行动、后续状态的期望奖励：

$$
r(s,a,s')=E[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s']\sum_{r\in R}{r\frac{p(s',r|s,a)}{p(s'|s,a)}}
$$

## 3.7 value functions

value function 用于量化评估不同条件下行动的好坏程度。我们可以对返回值求期望来作为value function来进行估计，agent的策略决定了如何来计算这一期望。策略就是有状态空间、行动空间到概率空间的一个映射，对于MDP，我们分别定义状态值函数为：在状态s下，遵守策略$\pi$的期望累积奖励，即：

$$
V_{\pi}(s)=E_{\pi}[G_t|S_t=s]=E_{\pi}\left[\sum_{k=0}^{\infty}{\gamma^kR_{t+k+1}|S_t=s}\right]
$$

定义状态-动作值函数为：在状态s下，采取行动a,遵守策略$\pi$的期望累积奖励，即：

$$
V_{\pi}(s)=E_{\pi}[G_t|S_t=s,A_t=a]=E_{\pi}\left[\sum_{k=0}^{\infty}{\gamma^kR_{t+k+1}|S_t=s,A_t=a}\right]
$$

### Monte Carlo Methods

* 在实验中，如果agent在遵守策略$\pi$的条件下，为每个出发状态s都保留了后面的真实返回值的均值，显然这个均值会随着实验次数的增加而最终收敛到$v_{\pi}(s)$。
* 类似地，如果还细分到每个出发状态s、每个行动a都保留了真实返回值的均值，则能最终收敛到$q_{\pi}(s,a)$
  但如果状态空间很大，进行MC模拟很不现实。此时即可将$v_{\pi},q_{\pi}$看作参数化函数，通过调参来逼近真实值，一样能够较为精确地估计。

### Bellman equation

$$
v_{\pi}(s)=\sum_{\alpha}{\pi(a|s)\sum_{s',r}{p(s',r|s,a)[r+\gamma v_{\pi}(s')]}}
$$

## 3.8 optimal value functions

$$
q_*(s,a)=E\[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a]
$$

### Bellman optimal equation

贝尔曼最优方程表示了最优策略下一个状态的状态值，必然等于该状态下最优行动的期望返回值：

$$
v_*(s)=max_{a\in \mathcal{A}(s)}q_{\pi_*}(s,a)=max_{a} \sum_{s',r}{p(s',r|s,a)[r+\gamma v_*(s')]}
$$

$$
q_*(s)= \sum_{s',r}p(s',r|s,a)[r+\gamma max_{a'}q_*(s')]
$$

```
最优策略：显然至少存在一个行动使行动值取到$v_*$,如果一个策略只将非0概率分配给这个行动，称这个策略为最优策略。
```

如果每一步都采取贪心策略，即只根据$v_*$来确定下一步的行动，这样的行动却恰好是最优行动，$v_*$的优美之处便体现于此。之所以能达到这一效果，是因为$v_*$已经考虑到了未来所有可能性，于是，看似贪心的「一步搜索」却能生成出全局最优行动。
如果我们进一步解得了$q_*$，agent 甚至都无需来做「一步搜索」：对于任意状态 s ，只需找到$a_0$使得$q_*(a_0,s)=max_{a}q_*(s,a)$即可。这是因为我们已经在之前的工作中多做了一些准备，将进一步的搜索信息缓存在了各个了$q_*$中，使得它的信息量比 $v_*$ 更大。

## optimality and approximation

前面已经讲过，最优策略是通过耗费极端的计算资源求得的，这使得我们不得不考虑一些方法来近似估计前面的一些函数。
在近似求解最优行为时，我们不难想象，会有很多状态其实只会以极低概率出现，我们若仍对其求解最优行动则意义不大，这时候如果选取一个局部最优行动来代替最优行动，显然，从整体期望意义来看，对总体奖励值造成的影响其实并不大，但却能节省很多的计算资源。与之对应的，当遇到那些经常出现的状态，我们则务必求出最优解。这是在解决 MDP 时强化学习方法区别于其他近似方法的一个重要性质。
